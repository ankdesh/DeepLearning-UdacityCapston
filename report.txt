I am hugely interested in AI and Computer vision seems to be a great stepping stone to start. This the reason I picked up this problem. This problem is neither too large like Imagenet that I need to spend weeks just to do an iteration of training hence limiting learning opportunities for debugging or improving the results. Nor is the problem as small as MNIST that gives a extremely good result inspite of model used (of course a deep model but not too deep).



Metrics

Since the applications for reading house number would require the number to be correct completely, we need to use the metric which considers the result to be correct only when the model predits all the digits to be correct (and correct number of digits). Further, we did a histogram for the digits at each position in the house numbers and the digits seems to almost uniformly distributed. This suggests that accuracy will be an appropriate metric and we dont need to do for F1 score (or precition and recall).

To measure the performance of the learned model, we need to use a metric which gives measure of how different the predicted output of model is from the actual number in the images. We use on 0-1 marking rule for a particular image i.e. if the prediction completely matches the real result, it is awarded 1 mark. In case the prediction is incorrect (even for a single digit), it is awarded 0 marks. We using this marking scheme over a large set of images and calculate the aggregated accuracy as

Accuracy = (Sum of marks based on above scheme) / (Total number of test cases)

For example if the Following are the predicted and actual digits 
| Predicted | Actual| Marks|
| 123| 123 | 1 |
| 125| 123 | 0 |
| 12 | 123 | 0 |
| 23 | 23  | 1 | 

Accuracy = 2/4 = 0.5




Algorithms and Techniques
Given the huge success of Convolutional Neural Network for image classification problems, it
makes CNNs to be ideal choice for this problem. The architecture and hyperparameters for the
model will be determined using experimentation using cross validations.

Convolutional neural networks are a class of neural networks where each neuron takes inputs from a fixed small window of neurons from previous layer, spatially located near each other. This helps each neuron to capture a arbitrary function of the features which are correlated spatially. One such example is images. Images have a high degree of spatial information. This is the reason CNNs have been extremely sucessful in image reated classification and regression tasks (all top entries in ImageNet competition are based on different architectures of CNNs. 

The typical type of layers involved in Convolution neural networks are 
- Convolutional layers - These layers perform a 2D(or 3D) convolution over the layers and each convolution operation leads to the linear combination of spatially located neurons. We use 3X3 convolutional filters for these layers (based on VGG architecture) and its philosophy that 3X3 applied repeatedly in consecutive layers is effectively 5X5 and 7X7 kernels with more interleaved non-linearities[3].
- Activation layers - These are layers which follow the convolution layers and introduce non-linearity to allow the system to capture non-linear relationships between neurons. We use ReLU activation layers which show a great performance without the diminishing gradient problems common in Sigmoidal and TanH functions. Thus allows deep models to be learned
- Max Pooling layers - These layers do downsampling from the earlier activation layers by selecting a neron with highest activation value from the a typically 2X2 window. These layers help to reduce overfitting and reducing paramters and computational requirements for a model. 
- Fully connected layers + Softmax layer - These layers are used for classification of the image based on the features it recieves from the complete network of earlier mentioned layers. In these layers, each neuron from a level is connected to every neuron from earlier layer. The activation generated by these fully connected layers is passed through a set of softmax neurons which maps the input activation to probability for each target class. We use the argmax of the probabalities to assign the class to the input image.

For optimization of the Convolution Neural Network, we use variant of gradient descent algorithm. Adam (Adaptive Moment Estimation) is a variant of gradient descent where momentum and learning rate are tracked and descreased exponentially. Adam is less sensitive to the learning rate set Altough Adam works great with most of the learning problems, no optimizer is universally better than the other. So we tried and evaluaged SGD and RMSProp during hyperparameter tuning.




The work required multiple iteration of debugging and expermenting to get to the working phase. Following issues were faced during the work. 
Two stage pipeline approach - During this approach after trying a lot simple CNN models, the accuracy remained at 10% (same as random chance). The problem later on turned out be that I was using full images to find the house numbers and the simple models were not able to capture the variance in the large dataset and was thus underfitting. Cropping the images to the Bounding boxes of the numbers solved this problem.
End to End approach - A similar problem occured while using VGG network where the model was stuck at 10%. The problem this time was high learning rate (0.1). Interestingly, while working on simpler model, this problem did not occur even with high learning rate and although the accuracy was stuck at ~70% (underfitting due to limited model capacity), it was better than getting stuck at 10%. The problem just disappeared when the learning rate was set at 0.01.






Most difficult part I encountered in project was the RNN approach. Recent papers have hinted that using RNNs can be advantageous for image recognitions problems too. Which I tried using for building a simple LSTM based classifier ( https://goo.gl/GAvPa1 ) for single digit recognition problem. However, could not get it working. I decided to master the CNNs first and then later learn to mix CNN and LSTM to imrove the results (some day !!!), similar to approach in [5]. 
Another issue which I should have had resolved earlier was using a pipeline to preprocess the data and dumping into a intermediate HDF5 file (rather than reading from the image file everytime). This speeded up my experiments by atleast 2-3 times faster and helped to do much more experiments.

The most interesting part was debugging using the tensorboard which allowed to debug the problems by visual inspection. TFlearn.org library generates a lot of default graphs and histograms which hints to the layer which has problem. It gave me a lot of insight into the CNNs that would have not been possible by just text log descriptions. This problem has set a confidence level for me to take up real life (or rather a bit refined Kaggle) problems related to computer vision.



[3] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).
[4] He, Pan, et al. "Reading scene text in deep convolutional sequences." arXiv preprint arXiv:1506.04395 (2015).


